{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__OqsFjXEDWj"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "# 假设文件路径为 'x-final.tar.gz'\n",
        "file_path = '/content/paws_wiki_labeled_final.tar.gz'\n",
        "extract_dir = './data'  # 解压到的目录\n",
        "# 解压文件\n",
        "with tarfile.open(file_path, 'r:gz') as tar:\n",
        "    tar.extractall(path=extract_dir)\n",
        "print(f\"文件解压到：{extract_dir}\")\n",
        "\n",
        "import pandas as pd\n",
        "# 假设解压后的文件夹中有一个开发集文件 en-dev.tsv\n",
        "data_file = os.path.join(extract_dir, '/content/data/final/dev.tsv'\n",
        "# 加载数据集\n",
        "df = pd.read_csv(data_file, sep='\\t')\n",
        "# 查看数据格式\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "0Pn3fA8HEQMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 数据预处理\n",
        "# 在加载数据后，我们可以进行文本清洗、分词、去除停用词、向量化等操作。例如，进行中文和英文文本的清洗和分词\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "# 假设使用英文文本进行预处理\n",
        "def clean_text(text):\n",
        "    # 清理HTML标签和非字母字符\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # 只保留字母和数字\n",
        "    return text\n",
        "\n",
        "def tokenize(text, language='english'):\n",
        "    if language == 'chinese':\n",
        "        return list(jieba.cut(text))\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# 对英文句子进行清洗和分词\n",
        "df['sentence1_clean'] = df['sentence1'].apply(clean_text)\n",
        "df['sentence2_clean'] = df['sentence2'].apply(clean_text)\n",
        "\n",
        "df['tokens1'] = df['sentence1_clean'].apply(lambda x: tokenize(x, language='english'))\n",
        "df['tokens2'] = df['sentence2_clean'].apply(lambda x: tokenize(x, language='english'))\n",
        "\n",
        "print(df[['sentence1', 'sentence2', 'tokens1', 'tokens2']].head())"
      ],
      "metadata": {
        "id": "iaWcz9CVEWor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# 使用TF-IDF向量化器\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# 将句子向量化\n",
        "X = vectorizer.fit_transform(df['sentence1_clean'] + \" \" + df['sentence2_clean'])\n",
        "\n",
        "print(X.shape)  # 查看向量化后的形状"
      ],
      "metadata": {
        "id": "FbBFv7ESEY9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 划分数据集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 创建SVM模型\n",
        "svm_model = SVC(kernel='linear')\n",
        "\n",
        "# 训练模型\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# 预测\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# 计算准确率\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Xom7UnnZEaYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# 假设我们有预处理好的文本序列（例如，通过BERT嵌入）\n",
        "def create_cnn_model(vocab_size, embedding_dim, input_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
        "    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    return model\n",
        "\n",
        "# 假设我们已经将句子对转化为数字序列\n",
        "vocab_size = 5000  # 词汇表大小\n",
        "embedding_dim = 100  # 词嵌入维度\n",
        "input_length = 100  # 输入长度（每个句子的最大词数）\n",
        "\n",
        "# 构建CNN模型\n",
        "cnn_model = create_cnn_model(vocab_size, embedding_dim, input_length)\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 假设 X_train_seq 和 X_test_seq 是经过数字化的训练和测试集\n",
        "# cnn_model.fit(X_train_seq, y_train, epochs=5, batch_size=32)\n",
        "# y_pred_cnn = cnn_model.predict(X_test_seq)"
      ],
      "metadata": {
        "id": "fXCr_gu4EdCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "KJkxv7RDEhJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import jieba\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "\n",
        "# # 加载数据集\n",
        "# data_file = os.path.join(extract_dir, '/content/data/final/train.tsv')\n",
        "# df = pd.read_csv(data_file, sep='\\t')\n",
        "# 加载数据集\n",
        "train_file = os.path.join(extract_dir, '/content/data/final/train.tsv')\n",
        "test_file = os.path.join(extract_dir, '/content/data/final/test.tsv')\n",
        "\n",
        "df_train = pd.read_csv(train_file, sep='\\t')\n",
        "df_test = pd.read_csv(test_file, sep='\\t')\n",
        "\n",
        "# 合并训练集和测试集\n",
        "df = pd.concat([df_train, df_test], ignore_index=True)\n",
        "\n",
        "# 数据预处理\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def tokenize(text, language='english'):\n",
        "    if language == 'chinese':\n",
        "        return list(jieba.cut(text))\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "df['sentence1_clean'] = df['sentence1'].apply(clean_text)\n",
        "df['sentence2_clean'] = df['sentence2'].apply(clean_text)\n",
        "\n",
        "# 分析词汇量和句子长度\n",
        "all_texts = df['sentence1_clean'].tolist() + df['sentence2_clean'].tolist()\n",
        "tokenizer = Tokenizer(num_words=None)\n",
        "tokenizer.fit_on_texts(all_texts)\n",
        "vocab_size = len(tokenizer.word_index) + 1  # +1 for the padding token\n",
        "sequences = tokenizer.texts_to_sequences(all_texts)\n",
        "max_length = max(len(seq) for seq in sequences)\n",
        "\n",
        "# CNN模型参数\n",
        "vocab_size = vocab_size  # 假设词汇表大小\n",
        "embedding_dim = 200\n",
        "max_length = max_length  # 假设最大序列长度\n",
        "\n",
        "# 文本向量化\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df['sentence1_clean'] + df['sentence2_clean'])\n",
        "sequences = tokenizer.texts_to_sequences(df['sentence1_clean'] + df['sentence2_clean'])\n",
        "X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "# 划分数据集\n",
        "# X_train_cnn, X_test_cnn, y_train, y_test = train_test_split(X, df['label'], test_size=0.2, random_state=42)\n",
        "# 划分数据集\n",
        "X_train_cnn, X_test_cnn, y_train, y_test = train_test_split(X, df['label'], test_size=0.139, random_state=42)\n",
        "\n",
        "# 构建改进的CNN模型\n",
        "cnn_model = Sequential()\n",
        "cnn_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))  # 改变卷积核大小\n",
        "cnn_model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))  # 添加额外的卷积层\n",
        "cnn_model.add(GlobalMaxPooling1D())\n",
        "cnn_model.add(Dense(64, activation='relu'))  # 添加全连接层\n",
        "cnn_model.add(tf.keras.layers.Dropout(0.5))  # 添加Dropout层\n",
        "cnn_model.add(Dense(1, activation='sigmoid'))  # 修改输出层为单个单元和sigmoid激活函数\n",
        "\n",
        "# 编译CNN模型\n",
        "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# 训练CNN模型以提取特征\n",
        "cnn_model.fit(X_train_cnn, y_train, epochs=50, batch_size=32, validation_split=0.01, verbose=1)\n",
        "\n",
        "# 提取CNN特征\n",
        "cnn_model.layers.pop()  # 移除最后的Dense层\n",
        "cnn_features_train = cnn_model.predict(X_train_cnn)\n",
        "cnn_features_test = cnn_model.predict(X_test_cnn)\n",
        "\n",
        "# 使用SVM进行分类\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(cnn_features_train, y_train)\n",
        "y_pred = svm_model.predict(cnn_features_test)\n",
        "\n",
        "# 保存分类结果\n",
        "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "results_df.to_csv('classification_results.csv', index=False)\n",
        "\n",
        "# 评估模型\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# 保存分类结果\n",
        "results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "results_df.to_csv('classification_results.csv', index=False)\n"
      ],
      "metadata": {
        "id": "DTH_kBkUEleh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 加载和处理 dev.tsv 和 test.tsv 文件\n",
        "def load_and_process_data(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t')\n",
        "    df['sentence1_clean'] = df['sentence1'].apply(clean_text)\n",
        "    df['sentence2_clean'] = df['sentence2'].apply(clean_text)\n",
        "    sequences = tokenizer.texts_to_sequences(df['sentence1_clean'] + df['sentence2_clean'])\n",
        "    X = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "    return X, df['label']\n",
        "\n",
        "dev_file = os.path.join(extract_dir, '/content/data/final/dev.tsv')\n",
        "test_file = os.path.join(extract_dir, '/content/data/final/test.tsv')\n",
        "\n",
        "X_dev, y_dev = load_and_process_data(dev_file)\n",
        "X_test, y_test = load_and_process_data(test_file)\n",
        "\n",
        "# 使用训练好的SVM模型对 dev 和 test 数据集进行预测\n",
        "y_pred_dev = svm_model.predict(X_dev)\n",
        "y_pred_test = svm_model.predict(X_test)\n",
        "\n",
        "# 保存 dev 和 test 的分类结果\n",
        "dev_results_df = pd.DataFrame({'Actual': y_dev, 'Predicted': y_pred_dev})\n",
        "test_results_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_test})\n",
        "dev_results_df.to_csv('dev_classification_results.csv', index=False)\n",
        "test_results_df.to_csv('test_classification_results.csv', index=False)\n",
        "\n",
        "# 评估 dev 和 test 数据集的模型性能\n",
        "print(\"Dev Set Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_dev, y_pred_dev):.4f}\")\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_dev, y_pred_dev))\n",
        "\n",
        "print(\"Test Set Performance:\")\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred_test):.4f}\")\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "db9K6R3lE5ci"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}